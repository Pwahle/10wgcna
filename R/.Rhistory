#Melt count data
countData <- melt(countData)
colnames(countData) <- c("gene", "sample", "count")
#set genotype
countData$genotype <- ifelse(grepl("wt", countData$sample, ignore.case = T), "wt",
ifelse(grepl("tf2", countData$sample, ignore.case = T), "tf2", "unknown"))
#set tissue
countData$tissue <- ifelse(grepl("other", countData$sample, ignore.case = T), "other",
ifelse(grepl("mbr", countData$sample, ignore.case = T), "mbr", "unknown"))
#Set Region
countData$region <- ifelse(grepl("a", countData$sample, ignore.case = T), "A",
ifelse(grepl("c", countData$sample, ignore.case = T), "C", "B"))
#Set type
countData$type <- paste(countData$region, countData$tissue,  sep = "")
head(countData)
```
###Most Significantly DE genes
```{r, eval = FALSE}
#you need to reset working directory to allSigGenes for easier import of list of significant genes
setwd("~/Dropbox/Research/LCMProject/08SOM/lcmSOM/data/allSigGenes/")
#makes a list of all files in directory
file_list <- list.files()
#Creates one list from all the files in directory
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("dataset")){
dataset <- read.table(file, header=TRUE, sep="\t")
}
# if the merged dataset does exist, append to it
if (exists("dataset")){
temp_dataset <-read.table(file, header=TRUE, sep="\t")
dataset<-rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
names(dataset)
dim(dataset)
allGenes<- dataset
#recieve just the list
allGenesITAG <- allGenes[,1]
length(allGenesITAG)
#Remove duplicates
allGenesITAG <- unique(allGenesITAG)
length(allGenesITAG)
#make an empty table to hold all the genes
allGeneList <- data.frame(t(rep(NA,7)))
colnames(allGeneList) <- c("type", "genotype", "N", "mean", "sd", "se", "gene")
allGeneList <- allGeneList[-1,] #remove first row
head(allGeneList)
```
#Loop together all relevent gene information.
```{r, eval = FALSE}
for(GENE in allGenesITAG) {
if(length(grep(GENE, countData$gene)) < 1){ #this is just making sure that the list of sig genes
next;
}
geneData <- subset(countData, grepl(GENE, countData$gene))
sumGraph <- ddply(geneData, c("type", "genotype"), summarise,
N    = length(count),
mean = mean(count),
sd   = sd(count),
se   = sd / sqrt(N))
sumGraph$gene <- GENE
allGeneList  <- rbind(allGeneList, sumGraph) #bind together all the new rows per loop.
}
mostDEgenes <- read.csv("../data/allGeneListBothGenotypes_analysis5b.csv")
mostDEgenes <- mostDEgenes[c(7, 2, 1, 4)] #keep only needed columns (gene, genotype, type, mean)
head(mostDEgenes)
#Change from long to wide data format
mostDEgene.long <- cast(mostDEgenes, genotype + gene ~ type, value.var = mean, fun.aggregate = "mean")  #why did I have to specify "mean" here? Are there duplicates of types? Double check later.
head(mostDEgene.long)
mostDEgene.long <- as.data.frame(mostDEgene.long)
mostDEgenes <- read.csv("../data/allGeneListBothGenotypes_analysis5b.csv")
setwd("~/Dropbox/Research/LCMProject/08SOM/lcmSOM/r")
mostDEgenes <- read.csv("../data/allGeneListBothGenotypes_analysis5b.csv")
mostDEgenes <- mostDEgenes[c(7, 2, 1, 4)] #keep only needed columns (gene, genotype, type, mean)
head(mostDEgenes)
#Change from long to wide data format
mostDEgene.long <- cast(mostDEgenes, genotype + gene ~ type, value.var = mean, fun.aggregate = "mean")  #why did I have to specify "mean" here? Are there duplicates of types? Double check later.
head(mostDEgene.long)
mostDEgene.long <- as.data.frame(mostDEgene.long)
names(mostDEgene.long)
scale_data <- as.matrix(t(scale(t(mostDEgene.long[c(3:8)]))))
head(scale_data)
#Principle Component Analysis
pca <- prcomp(scale_data, scale=TRUE)
summary(pca)
pca.scores <- data.frame(pca$x)
data.val <- cbind(mostDEgene.long, scale_data, pca.scores)
head(data.val)
```
##Visualizing the PCA
Looks to be three major clusters.
```{r}
p <- ggplot(data.val, aes(PC1, PC2))
p + geom_point()
```
##SuperSOM
```{r}
set.seed(6)
names(data.val)
superSomData <- data.val[,c(1:8)]
tf2 <- subset(superSomData, genotype == "tf2", select = 3:8)
wt <- subset(superSomData, genotype == "wt", select = 3:8)
wt <- as.matrix(wt)
tf2 <- as.matrix(tf2)
sc.wt <- t(scale(t(wt)))
sc.tf2 <- t(scale(t(tf2)))
all.data <- list(sc.wt,sc.tf2)
ssom <- supersom(all.data, somgrid(6, 6, "hexagonal"),weights=c(0.5,0.5))
summary(ssom)
par(mfrow = c(3, 2))
plot(ssom, type ="changes")
plot(ssom, type = "codes")
plot(ssom, type = "counts")
plot(ssom, type = "quality")
data.val <- cbind(data.val,ssom$unit.classif,ssom$distances)
head(data.val)
write.table(data.val, file="../data/ssom.data.analysis5d.txt")
```
###Visualization
```{r}
plot.data <- read.table("../data/ssom.data.analysis5d.txt",header=TRUE)
names(plot.data)
dim(plot.data)
p <- ggplot(plot.data, aes(PC1, PC2, colour=factor(ssom.unit.classif)))
p + geom_point() + theme_bw()
```
```{r}
##Output for network
sub_cluster <- subset(plot.data, ssom.unit.classif==12)
write.csv(sub_cluster, "../data/superSOM_analysis8.csv")
genes25 <- read.csv("../data/analysis4.top25.csv")
genes25 <- genes25[,c(2,9:14)]
```
```{r}
scale_data <- as.matrix(t(scale(t(genes25[c(2:7)]))))
pca <- prcomp(scale_data, scale=TRUE)
summary(pca)
pca.scores <- data.frame(pca$x)
data.val <- cbind(genes25, scale_data, pca.scores)
```
##Visualizing the PCA
```{r}
p <- ggplot(data.val, aes(PC1, PC2))
p + geom_point()
```
##Self Organizing Map - (6,6), large
```{r}
#subset only the scaled gene expression values
set.seed(6)
som <- som(data=scale_data, somgrid(6,6,"hexagonal")) # This is where you change the size of the map
summary(som)
```
###Look at the SOM results
```{r}
plot(som, type = "codes")
plot(som, type = "counts")
plot(som, type="dist.neighbours")
som$data <- data.frame(som$data) #changed to dataframe to extract column names easier.
data.val2 <- cbind(data.val,som$unit.classif,som$distances)
#fix to one regex
data.val2$gene <- gsub("^(.*)[.].*", "\\1", data.val2$gene)
data.val2$gene <- gsub("^(.*)[.].*", "\\1", data.val2$gene)
head(data.val2)
#I need to add in the region to this information for subsetting by region when visualizing. In order to do that, I need to melt, add region information then cast back out.
```
Upload the gene expression list.
```{r}
geneList1 <- read.csv("../../../06diffGeneExp/analysis/indvGenes/yasuCuratedGenes/pnas.1402835111.sd06.csv")
#isolate the genes
genesOfInterest <- geneList1[,c(1,3)]
colnames(genesOfInterest) <- c("gene", "name")
names(genesOfInterest) #check
```
```{r}
#This is a ridiculas around assigning if a gene is a curated gene!
#Figure out more elegent way.
data.val2$curated <- match(data.val2$gene, genesOfInterest$gene)
data.val2$curated <- gsub("[[:digit:]]+", "yes", data.val2$curated)
data.val2$curated[is.na(data.val2$curated)] <- "no"
```
Visualize the major clusters.  Here are the leaf curated genes in PC space.
```{r}
head(data.val2)
p <- ggplot(data.val2, aes(PC1, PC2, color = curated))
p + geom_point(size=I(2), alpha = 0.6) +
scale_colour_manual(values=c("#cccccc", "#33ff33")) +
theme_bw()
```
###Visualize by Cluster
Read in data used for GO enrichment analysis.
```{r}
geneLength <- read.csv("../../../07GO_enrichment/requisiteData/normalized_genes_length.csv")
cate <- read.table("../../../07GO_enrichment/requisiteData/melted.GOTable.txt",header=TRUE)
sub_cluster4 <- subset(plot.data, ssom.unit.classif==4)
sub_cluster6 <- subset(plot.data, ssom.unit.classif==6)
sub_cluster17 <- subset(plot.data, ssom.unit.classif==17)
sub_cluster19 <- subset(plot.data, ssom.unit.classif==19)
sub_cluster20 <- subset(plot.data, ssom.unit.classif==20)
sub_cluster <- rbind(sub_cluster4, sub_cluster6, sub_cluster17, sub_cluster19, sub_cluster20)
write.csv(sub_cluster, "../data/superSOM_analysis9.5.csv")
write.csv(sub_cluster, "../data/SOM_analysis9.5.csv")
sub_cluster <- rbind(sub_cluster4, sub_cluster6, sub_cluster17, sub_cluster19, sub_cluster20)
setwd("~/Dropbox/Research/LCMProject/10wgnca/R")
superSOM <- read.csv("../../08SOM/lcmSOM/data/forNetwork/SOM_analysis9.5.csv")
SOM <- read.csv("../../08SOM/lcmSOM/data/forNetwork/SOM_analysis9.5.csv")
superSOM <- read.csv("../../08SOM/lcmSOM/data/forNetwork/superSOM_analysis8.csv")
dim(SOM)
colnames(SOM)
dim(superSOM)
colnames(SOM)
SOMclusters <- rbind(superSOM, SOM)
dim(SOMclusters)
dim(SOMclusters)
genes=rownames(counts)
dim(SOMclusters)
dim(SOMclusters)
colnames(SOM)
head(SOMclusters)
coolGenes <- SOMclusters$gene
head(coolGenes)
head(SOMclusters)
coolGenes <- SOMclusters$gene
summary(coolGenes)
colnames(counts)
rownames(counts)
summary(coolGenes)
?merge
counts <- read.csv("../data/pnas.1402835111.sd01.csv")
dim(counts)
head(counts)
counts <- read.csv("../data/pnas.1402835111.sd01.csv",row.names=1)
merge(coolGenes, counts, by = "gene")
merge(coolGenes, counts, by.x = "gene")
rownames(counts)
coolGenes
counts[1]
rowname(counts[1])
rownames(counts[1])
rownames(counts[1]) <- "gene"
rownames(counts)[1] <- "gene"
head(counts)
merge(coolGenes, counts, by.x = "gene")
head(coolGenes)
coolGenes <- as.table(SOMclusters$gene)
head(SOMclusters)
coolGenes <- SOMclusters[,c(2:3)]
coolGenes
merge(coolGenes, counts, by.x = "gene")
head(coolGenes)
head(counts)
head(coolGenes)
merge(coolGenes, counts, by.x = "gene")
merge(coolGenes, counts, by.y = "gene")
merge(coolGenes, counts, by.y = gene)
merge(coolGenes, counts, by = "gene")
head(coolGenes)
head(counts)
counts <- read.csv("../data/pnas.1402835111.sd01.csv")
dim(counts)
head(counts)
colnames(counts)
counts <- read.csv("../data/pnas.1402835111.sd01.csv")
dim(counts)
head(counts)
colnames(counts)
colnames(counts)[1] <- "gene"
head(coolGenes)
head(counts)
merge(coolGenes, counts, by = "gene")
merged <- merge(coolGenes, counts, by = "gene")
head(merged)
merged[,-c(2)]
counts <- merged[,-c(2)]
colnames(merged)
colnames
colnames(counts)
colnames(counts)
rownames(counts) <- counts[,1]
colnames(counts)
countsUniq <- unique(counts[,1])
head(countsUniq)
countsUniq <- unique(counts)
dim(counts)
dim(countsUniq)
rownames(counts) <- counts[,1]
rownames(countsUniq) <- countsUniq[,1]
counts <- countsUniq
genes=rownames(counts)
counts.t <- t(counts)
B=100  ## select number of bootstrap resamples
powers  =  c(c(3:50)) #if you get error in the bootsrapping, you might need to the maximum value here.
result=matrix(nrow=ncol(counts.t), ncol=B)
for (i in 1:B){
set.seed(i*100+1)
print(i)
##bootstrap resample
sft.power=30
while(sft.power>29 || is.na(sft.power)){#because TOM need power < 30 #softconnecity power < 14
index.b=sample(x=1:nrow(counts.t), size=nrow(counts.t), replace=TRUE)
Y.b=counts.t[index.b,]
##soft thresholding
sft.b = pickSoftThreshold(Y.b,  powerVector=powers, RsquaredCut=0.9, verbose  =  5)
sft.power = sft.b$powerEstimate
}
print(sft.power)
##TOM
TOM.b = TOMsimilarityFromExpr(Y.b,power=sft.b$powerEstimate) #omega TOM-based connectivity
hub.b = rowSums(TOM.b)
#adj.b = adjacency(Y.b,power=sft.b$powerEstimate)
#hub.b = rowSums(adj.b) #k connectivity
result[,i]<-rank(-hub.b)
}
counts <- countsUniq
head(counts)
counts.t <- t(counts)
B=100  ## select number of bootstrap resamples
powers  =  c(c(3:50)) #if you get error in the bootsrapping, you might need to the maximum value here.
result=matrix(nrow=ncol(counts.t), ncol=B)
for (i in 1:B){
set.seed(i*100+1)
print(i)
##bootstrap resample
sft.power=30
while(sft.power>29 || is.na(sft.power)){#because TOM need power < 30 #softconnecity power < 14
index.b=sample(x=1:nrow(counts.t), size=nrow(counts.t), replace=TRUE)
Y.b=counts.t[index.b,]
##soft thresholding
sft.b = pickSoftThreshold(Y.b,  powerVector=powers, RsquaredCut=0.9, verbose  =  5)
sft.power = sft.b$powerEstimate
}
print(sft.power)
##TOM
TOM.b = TOMsimilarityFromExpr(Y.b,power=sft.b$powerEstimate) #omega TOM-based connectivity
hub.b = rowSums(TOM.b)
#adj.b = adjacency(Y.b,power=sft.b$powerEstimate)
#hub.b = rowSums(adj.b) #k connectivity
result[,i]<-rank(-hub.b)
}
row.names(result) <- genes
average <- rowMeans(result)
for (i in 1:B){
set.seed(i*100+1)
print(i)
##bootstrap resample
sft.power=30
while(sft.power>29 || is.na(sft.power)){#because TOM need power < 30 #softconnecity power < 14
index.b=sample(x=1:nrow(counts.t), size=nrow(counts.t), replace=TRUE)
Y.b=counts.t[index.b,]
##soft thresholding
sft.b = pickSoftThreshold(Y.b,  powerVector=powers, RsquaredCut=0.9, verbose  =  5)
sft.power = sft.b$powerEstimate
}
print(sft.power)
##TOM
TOM.b = TOMsimilarityFromExpr(Y.b,power=sft.b$powerEstimate) #omega TOM-based connectivity
hub.b = rowSums(TOM.b)
#adj.b = adjacency(Y.b,power=sft.b$powerEstimate)
#hub.b = rowSums(adj.b) #k connectivity
result[,i]<-rank(-hub.b)
}
B=100  ## select number of bootstrap resamples
powers  =  c(c(3:50)) #if you get error in the bootsrapping, you might need to the maximum value here.
result=matrix(nrow=ncol(counts.t), ncol=B)
for (i in 1:B){
set.seed(i*100+1)
print(i)
##bootstrap resample
sft.power=30
while(sft.power>29 || is.na(sft.power)){#because TOM need power < 30 #softconnecity power < 14
index.b=sample(x=1:nrow(counts.t), size=nrow(counts.t), replace=TRUE)
Y.b=counts.t[index.b,]
##soft thresholding
sft.b = pickSoftThreshold(Y.b,  powerVector=powers, RsquaredCut=0.9, verbose  =  5)
sft.power = sft.b$powerEstimate
}
print(sft.power)
##TOM
TOM.b = TOMsimilarityFromExpr(Y.b,power=sft.b$powerEstimate) #omega TOM-based connectivity
hub.b = rowSums(TOM.b)
#adj.b = adjacency(Y.b,power=sft.b$powerEstimate)
#hub.b = rowSums(adj.b) #k connectivity
result[,i]<-rank(-hub.b)
}
?pickSoftThreshold()
?pickSoftThreshold()
head(counts)
dim(coutns)
dim(counts)
counts.t <- t(counts)
B=100  ## select number of bootstrap resamples
powers  =  c(c(3:50)) #if you get error in the bootsrapping, you might need to the maximum value here.
result=matrix(nrow=ncol(counts.t), ncol=B)
print(i)
sft.power=30
while(sft.power>29 || is.na(sft.power)){#because TOM need power < 30 #softconnecity power < 14
index.b=sample(x=1:nrow(counts.t), size=nrow(counts.t), replace=TRUE)
Y.b=counts.t[index.b,]
##soft thresholding
sft.b = pickSoftThreshold(Y.b,  powerVector=powers, RsquaredCut=0.9, verbose  =  5)
sft.power = sft.b$powerEstimate
}
ALLOW_WGCNAT_THREADS = 4
library(WGCNA)
options(stringsAsFactors  =  FALSE)
#enableWGCNAThreads()
ALLOW_WGCNAT_THREADS = 4
library (igraph)
library(ggplot2)
library(reshape)
B=100  ## select number of bootstrap resamples
powers  =  c(c(3:50)) #if you get error in the bootsrapping, you might need to the maximum value here.
result=matrix(nrow=ncol(counts.t), ncol=B)
for (i in 1:B){
set.seed(i*100+1)
print(i)
##bootstrap resample
sft.power=30
while(sft.power>29 || is.na(sft.power)){#because TOM need power < 30 #softconnecity power < 14
index.b=sample(x=1:nrow(counts.t), size=nrow(counts.t), replace=TRUE)
Y.b=counts.t[index.b,]
##soft thresholding
sft.b = pickSoftThreshold(Y.b,  powerVector=powers, RsquaredCut=0.9, verbose  =  5)
sft.power = sft.b$powerEstimate
}
print(sft.power)
##TOM
TOM.b = TOMsimilarityFromExpr(Y.b,power=sft.b$powerEstimate) #omega TOM-based connectivity
hub.b = rowSums(TOM.b)
#adj.b = adjacency(Y.b,power=sft.b$powerEstimate)
#hub.b = rowSums(adj.b) #k connectivity
result[,i]<-rank(-hub.b)
}
## WGCNA with bootstrap
## 2014 1 21
## library
library(WGCNA)
options(stringsAsFactors  =  FALSE)
#enableWGCNAThreads()
ALLOW_WGCNAT_THREADS = 4
library (igraph)
library(ggplot2)
library(reshape)
## Read in data: dodder cluster 5 with indicidual biological replicates
## because WGCNA with soft threshold need > 12 samples
counts <- read.csv("../data/pnas.1402835111.sd01.csv")
dim(counts)
head(counts)
colnames(counts)
#at this point do I need to get rid of all the hab and pen samples?
#do I need to subset based on the genes I am interested in? Yes.
#Read in lists of genes
SOM <- read.csv("../../08SOM/lcmSOM/data/forNetwork/SOM_analysis9.5.csv")
dim(SOM)
colnames(SOM)
superSOM <- read.csv("../../08SOM/lcmSOM/data/forNetwork/superSOM_analysis8.csv")
dim(superSOM)
colnames(SOM)
#bring together lists from
SOMclusters <- rbind(superSOM, SOM)
dim(SOMclusters)
head(SOMclusters)
coolGenes <- SOMclusters[,c(2:3)]
coolGenes
#Now merge with Yasu's table to get only genes I am interested in.
#First rename 1st column in counts to gene for merging
colnames(counts)[1] <- "gene"
head(coolGenes)
head(counts)
#Merge
merged <- merge(coolGenes, counts, by = "gene")
colnames(merged)
counts <- merged[,-c(2)]
colnames(counts)
dim(counts)
#remove rows that the have duplicate gene names.
countsUniq <- unique(counts)
#Over half were duplicates.  Make sure you understand where these came from.
#There shouldn't be duplicates within each SOM
#change the gene column back to rownames
rownames(countsUniq) <- countsUniq[,1]
counts <- countsUniq
head(counts)
dim(counts)
counts.t <- t(counts)
#counts.lt=t(log(counts+1)) #Is this if you didn't normalize yet?
############################################################
## Bootstrapping for hub gene prediction
B=100  ## select number of bootstrap resamples
powers  =  c(c(3:50)) #if you get error in the bootsrapping, you might need to the maximum value here.
result=matrix(nrow=ncol(counts.t), ncol=B)
for (i in 1:B){
set.seed(i*100+1)
print(i)
##bootstrap resample
sft.power=30
while(sft.power>29 || is.na(sft.power)){#because TOM need power < 30 #softconnecity power < 14
index.b=sample(x=1:nrow(counts.t), size=nrow(counts.t), replace=TRUE)
Y.b=counts.t[index.b,]
##soft thresholding
sft.b = pickSoftThreshold(Y.b,  powerVector=powers, RsquaredCut=0.9, verbose  =  5)
sft.power = sft.b$powerEstimate
}
print(sft.power)
##TOM
TOM.b = TOMsimilarityFromExpr(Y.b,power=sft.b$powerEstimate) #omega TOM-based connectivity
hub.b = rowSums(TOM.b)
#adj.b = adjacency(Y.b,power=sft.b$powerEstimate)
#hub.b = rowSums(adj.b) #k connectivity
result[,i]<-rank(-hub.b)
}
